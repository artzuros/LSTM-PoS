{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('treebank')\n",
    "# nltk.download('universal_tagset')\n",
    "\n",
    "# Load the data\n",
    "sentences = treebank.tagged_sents(tagset='universal')\n",
    "train_data = sentences[:3000]\n",
    "test_data = sentences[3000:4000]\n",
    "\n",
    "vocab = {word for sent in train_data for word, tag in sent}\n",
    "vocab = {word: i+2 for i, word in enumerate(sorted(vocab))}\n",
    "vocab['<pad>'] = 0  # Padding\n",
    "vocab['<unk>'] = 1  # Unknown words\n",
    "\n",
    "tagset = {tag for sent in train_data for word, tag in sent}\n",
    "tagset = {tag: i for i, tag in enumerate(tagset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CONJ': 0,\n",
       " 'PRT': 1,\n",
       " 'ADP': 2,\n",
       " 'X': 3,\n",
       " 'ADJ': 4,\n",
       " 'VERB': 5,\n",
       " 'ADV': 6,\n",
       " 'NOUN': 7,\n",
       " '.': 8,\n",
       " 'PRON': 9,\n",
       " 'DET': 10,\n",
       " 'NUM': 11}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8711"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['quick']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tagset):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.tagset = tagset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = zip(*self.data[idx])\n",
    "        word_ids = [self.vocab.get(word, self.vocab['<unk>']) for word in words]\n",
    "        tag_ids = [self.tagset[tag] for tag in tags]\n",
    "        return torch.tensor(word_ids, dtype=torch.long), torch.tensor(tag_ids, dtype=torch.long)\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = nn.utils.rnn.pad_sequence(xx, batch_first=True, padding_value=vocab['<pad>'])\n",
    "    yy_pad = nn.utils.rnn.pad_sequence(yy, batch_first=True, padding_value=-1)  # -1 for ignore_index in loss calculation\n",
    "    return xx_pad, yy_pad, torch.tensor(x_lens, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(POSDataset(train_data, vocab, tagset), batch_size=32, collate_fn=pad_collate)\n",
    "test_loader = DataLoader(POSDataset(test_data, vocab, tagset), batch_size=32, collate_fn=pad_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Gates parameters\n",
    "        self.W_i = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.U_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.W_f = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.U_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.W_c = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.U_c = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.W_o = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.U_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        x: Shape (batch, sequence, feature)\n",
    "        init_states: (h, c) initial states\n",
    "        \"\"\"\n",
    "        batch_size, seq_size, _ = x.size()\n",
    "        hidden_seq = []\n",
    "\n",
    "        if init_states is None:\n",
    "            h_t, c_t = (torch.zeros(batch_size, self.hidden_dim).to(x.device), \n",
    "                        torch.zeros(batch_size, self.hidden_dim).to(x.device))\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "\n",
    "        for t in range(seq_size):\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            i_t = torch.sigmoid(x_t @ self.W_i + h_t @ self.U_i + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.W_f + h_t @ self.U_f + self.b_f)\n",
    "            g_t = torch.tanh(x_t @ self.W_c + h_t @ self.U_c + self.b_c)\n",
    "            o_t = torch.sigmoid(x_t @ self.W_o + h_t @ self.U_o + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "\n",
    "        return hidden_seq, (h_t, c_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSModel(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim):\n",
    "        super(POSModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<pad>'])\n",
    "        self.lstm = CustomLSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, words, word_lengths):\n",
    "        embeds = self.embedding(words)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, lstm_out.shape[2])\n",
    "        tag_space = self.fc(lstm_out)\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores.view(words.size(0), words.size(1), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "model = POSModel(len(vocab), len(tagset), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore the padding in the loss calculation\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, loss_function, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for words, tags, lengths in train_loader:\n",
    "            model.zero_grad()\n",
    "            tag_scores = model(words, lengths)\n",
    "            loss = loss_function(tag_scores.view(-1, len(tagset)), tags.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, sentence, vocab, tagset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = [vocab.get(word.lower(), vocab['<unk>']) for word in sentence.split()]\n",
    "        inputs_tensor = torch.tensor(inputs).unsqueeze(0)\n",
    "        tag_scores = model(inputs_tensor, torch.tensor([len(inputs)]))\n",
    "        tag_ids = torch.argmax(tag_scores, dim=2)\n",
    "        tags = [list(tagset.keys())[tag_id] for tag_id in tag_ids[0]]\n",
    "        return list(zip(sentence.split(), tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 65.80699542164803\n",
      "Epoch 2, Loss: 19.28850381821394\n",
      "Epoch 3, Loss: 7.312714505940676\n",
      "Epoch 4, Loss: 3.2961814729496837\n",
      "Epoch 5, Loss: 1.8418646750506014\n",
      "Epoch 6, Loss: 1.1415140028111637\n",
      "Epoch 7, Loss: 0.7769635362783447\n",
      "Epoch 8, Loss: 0.5403456123895012\n",
      "Epoch 9, Loss: 0.3901566627318971\n",
      "Epoch 10, Loss: 0.3201424130820669\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, optimizer, loss_function, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('quick', 'ADJ'), ('brown', 'NOUN'), ('fox', 'NOUN'), ('jumps', 'NOUN'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'NOUN'), ('dog', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "tagged_sentence = test_model(model, sentence, vocab, tagset)\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = 'pos_model.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, vocab_size, tagset_size, embedding_dim, hidden_dim):\n",
    "    model = POSModel(vocab_size, tagset_size, embedding_dim, hidden_dim)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'NOUN'), ('very', 'CONJ'), ('good', 'NUM'), ('man', 'ADJ')]\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model_loaded = load_model('pos_model.pth', len(vocab), len(tagset), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "\n",
    "# Test the model on a new sentence\n",
    "sentence = \"A very good man\"\n",
    "tagged_sentence = test_model(model_loaded, sentence, vocab, tagset)\n",
    "print(tagged_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
